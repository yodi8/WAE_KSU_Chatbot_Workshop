{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’» WAE Chatbot Workshop"
      ],
      "metadata": {
        "id": "XdXkYweB4fKe"
      },
      "id": "XdXkYweB4fKe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Required Libraries\n",
        "We will use:\n",
        "\n",
        "Gradio for building simple user interfaces.\n",
        "\n",
        "Mistral AI for accessing the language model.\n"
      ],
      "metadata": {
        "id": "NNGEtyAg50vF"
      },
      "id": "NNGEtyAg50vF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0eb939e-a7e6-42d9-a7ce-c61444c5dc62",
      "metadata": {
        "id": "e0eb939e-a7e6-42d9-a7ce-c61444c5dc62",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install mistralai\n",
        "!pip install gradio\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output() # Clear previous output for a cleaner notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b630861-a95e-4925-8525-d9461d3627ea",
      "metadata": {
        "id": "5b630861-a95e-4925-8525-d9461d3627ea"
      },
      "source": [
        "Our API is currently available through [La Plateforme](https://console.mistral.ai/). You need to activate payments on your account to enable your API keys. After activation, you can start using the `chat` endpoint to interact with models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Setting up the Mistral Model\n",
        "\n",
        "You will need to add your API Key from:  https://console.mistral.ai/api-keys/\n",
        "\n",
        "\n",
        "You can explore all available models here: https://docs.mistral.ai/getting-started/models/models_overview/\n",
        "\n"
      ],
      "metadata": {
        "id": "nBnI4U-ClTYM"
      },
      "id": "nBnI4U-ClTYM"
    },
    {
      "source": [
        "from mistralai import Mistral\n",
        "\n",
        "api_key = \"YOUR_API_KEY_HERE\" # ðŸ‘‰ Replace with your own API Key\n",
        "model = \"mistral-large-2402\"  # You can change the model if you want\n",
        "\n",
        "client = Mistral(api_key=api_key)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zHUx1oep34i4"
      },
      "id": "zHUx1oep34i4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple Chatbot using Mistral Model"
      ],
      "metadata": {
        "id": "gClQXFFt6ahF"
      },
      "id": "gClQXFFt6ahF"
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_mistral(message, history):\n",
        "    # Build the conversation context by combining the existing history with the new user message.\n",
        "    messages = history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages # Content from the user input\n",
        "    )\n",
        "    response = chat_response.choices[0].message.content\n",
        "\n",
        "    formatted_response = {\"role\": \"assistant\", \"content\": response}\n",
        "\n",
        "    # Return only the new assistant message.\n",
        "    return formatted_response"
      ],
      "metadata": {
        "id": "dEYiOJZglZhC"
      },
      "id": "dEYiOJZglZhC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Web Interface using Gradio"
      ],
      "metadata": {
        "id": "FBV_HsRC6ylF"
      },
      "id": "FBV_HsRC6ylF"
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "iface = gr.ChatInterface(\n",
        "    fn=chat_with_mistral, # Function to handle chat messages\n",
        "    type=\"messages\", # Set the input/output format to handle full chat messages\n",
        "    title=\"Mistral AI Chatbot\",\n",
        "    description=\"Chat with the Mistral AI large language model.\",\n",
        "    theme=\"glass\", # Visual theme\n",
        ")\n",
        "\n",
        "iface.launch(\n",
        "    debug=True, # Show detailed error messages if something goes wrong\n",
        "    share=True, # Create a public link to share your app with others\n",
        ")"
      ],
      "metadata": {
        "id": "M1WtoPUl0L57"
      },
      "id": "M1WtoPUl0L57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ›  Exercise: E-commerce Store Chatbot\n",
        "You are building a chatbot for an online store that sells electronics (phones, laptops, and accessories).\n",
        "\n",
        "The chatbot should answer any questions related to the storeâ€™s products and services.\n",
        "\n",
        "If users ask about topics not related to the store or its offerings, the chatbot should politely inform them that it can only assist with store-related inquiries."
      ],
      "metadata": {
        "id": "GsgVEFFECVzp"
      },
      "id": "GsgVEFFECVzp"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the system prompt here\n",
        "system_prompt = {???}  # ðŸ‘‰ Here you should put 2 elements..\n",
        "\n",
        "# 2. Create the chat function\n",
        "def chat_with_store_bot(user_message, history):\n",
        "    ???\n",
        "\n",
        "# 3. Create the interface\n",
        "store_iface = gr.ChatInterface(???) # ðŸ‘‰ Only two parameters are necessary: 'fn' (function name) and 'type' (set it to \"messages\").\n",
        "\n",
        "# 4. Launch the interface\n",
        "store_iface.launch()"
      ],
      "metadata": {
        "id": "7KYGCjouDIaj"
      },
      "id": "7KYGCjouDIaj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ’¡ Hint\n",
        "\n",
        "<details>\n",
        "<summary>Click here to see the hint</summary>\n",
        "\n",
        "When writing your system prompt, follow this structure:\n",
        "\n",
        "```python\n",
        "{\"role\": \"system\", \"content\": \"Your special instruction goes here.\"}\n"
      ],
      "metadata": {
        "id": "tV2VPfZ6Crgl"
      },
      "id": "tV2VPfZ6Crgl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "&nbsp;  \n",
        "&nbsp;  \n",
        "&nbsp;"
      ],
      "metadata": {
        "id": "GdB3oV2mL6t_"
      },
      "id": "GdB3oV2mL6t_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Chatbot using Hugging Face Transformers\n",
        "\n",
        "## ðŸ¤– About the microsoft/DialoGPT-medium Model\n",
        "\n",
        "In this section, we introduce the **`microsoft/DialoGPT-medium`** model â€” a small open-domain chatbot developed by Microsoft and trained on casual Reddit conversations.  \n",
        "Unlike the **`mistral-large-2402`** model we used earlier, which is a large instruction-following language model designed for a wide range of complex tasks, **DialoGPT-medium** is much smaller and specialized mainly for casual, free-flowing conversations without strict understanding or task execution.  \n",
        "As a result, you may notice that **DialoGPT's responses are often less focused, more random, and sometimes humorous**, compared to the more structured and instruction-driven answers generated by Mistral.  \n",
        "This will give you a practical sense of how **different training objectives and model sizes** impact the quality and behavior of chatbots.  \n",
        "\n",
        "Chatbot fine-tuning will not be covered in this workshop, as it is a challenging task that typically requires deep expertise, significant resources, and specialized datasets."
      ],
      "metadata": {
        "id": "IWQWjPoM7oyx"
      },
      "id": "IWQWjPoM7oyx"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "gWSksJ8r7neV"
      },
      "id": "gWSksJ8r7neV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"microsoft/DialoGPT-medium\"  # Choose a chatbot model from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) # tokenizer: converts text into tokens that the model can understand\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name) # model: generates text responses based on the input tokens"
      ],
      "metadata": {
        "id": "Vpq4tK087WLF"
      },
      "id": "Vpq4tK087WLF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chat function\n",
        "def chat_with_dialoGPT(user_input, chat_history_ids=None):\n",
        "    # Encode the user input and add end-of-sentence token\n",
        "    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # If there is history, append the new input to history; else, just use the new input\n",
        "    if chat_history_ids is not None:\n",
        "        input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1)\n",
        "    else:\n",
        "        input_ids = new_input_ids\n",
        "\n",
        "    # Generate the model's response\n",
        "    chat_history_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=1000,               # maximum number of tokens for input + output\n",
        "        pad_token_id=tokenizer.eos_token_id,  # Padding token (using EOS token, which marks end of sentence)\n",
        "        do_sample=True,                # Enable sampling to create fun, more random responses\n",
        "        top_p=0.92,                    # Nucleus sampling (only pick from top 92% most likely next words)\n",
        "        temperature=0.75,              # Control randomness (slightly creative), higher values = more creative\n",
        "        repetition_penalty=1.1,         # Slightly discourage the model from repeating itself\n",
        "        no_repeat_ngram_size=3          # Prevent repeating any sequence of 3 words to make conversation sound more natural\n",
        "    )\n",
        "\n",
        "    # Decode the model's response\n",
        "    bot_response = tokenizer.decode(chat_history_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    return bot_response, chat_history_ids"
      ],
      "metadata": {
        "id": "Vte4wypYqQ7e"
      },
      "id": "Vte4wypYqQ7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize conversation history (for DialoGPT, it's token IDs history)\n",
        "chat_history_ids = None\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"quit\", \"q\", \"exit\"]:\n",
        "        print(\"Chat ended.\")\n",
        "        break\n",
        "\n",
        "    response, chat_history_ids = chat_with_dialoGPT(user_input, chat_history_ids)\n",
        "    print(f\"Chatbot: {response}\")"
      ],
      "metadata": {
        "id": "uNiWowBouIne"
      },
      "id": "uNiWowBouIne",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ’¬ **Note:**  \n",
        "> Of course, you can use **Gradio** to build a web-based chatbot interface as we did in the previous example. However, for the purpose of showing different methods, we are using the simple input/output approach."
      ],
      "metadata": {
        "id": "1UkmNOZHK4Cr"
      },
      "id": "1UkmNOZHK4Cr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ›  Exercise: Try a Different Hugging Face Model!\n",
        "\n",
        "You can find many pre-trained models on the [Hugging Face Model Hub](https://huggingface.co/models).\n",
        "To try a different model, follow these steps:\n",
        "1. Visit the Hugging Face Models page.\n",
        "2. Search for a model that matches your goal (for example, chatbot, text generation, question answering).\n",
        "3. Copy the model name exactly as shown (for example, `microsoft/DialoGPT-medium`).\n",
        "4. Replace the old model name inside your `AutoTokenizer.from_pretrained()` and `AutoModelForCausalLM.from_pretrained()` code.\n",
        "5. Test your chatbot again to see how different models behave!"
      ],
      "metadata": {
        "id": "C-0S3RlKFptj"
      },
      "id": "C-0S3RlKFptj"
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = ???\n",
        "tokenizer = ???\n",
        "model = ???\n",
        "\n",
        "def chat_with_your_model(user_input, chat_history_ids=None):\n",
        "  ???"
      ],
      "metadata": {
        "id": "PgRHAEDXFwpz"
      },
      "id": "PgRHAEDXFwpz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize conversation history (for DialoGPT, it's token IDs history)\n",
        "chat_history_ids = None\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"quit\", \"q\", \"exit\"]:\n",
        "        print(\"Chat ended.\")\n",
        "        break\n",
        "\n",
        "    response, chat_history_ids = ???\n",
        "    print(f\"Chatbot: {response}\")"
      ],
      "metadata": {
        "id": "aSGh-QRoKnND"
      },
      "id": "aSGh-QRoKnND",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ‰ Congratulations\n",
        "\n",
        "Congratulations on completing the workshop!\n",
        "\n",
        "Today, you learned how to build chatbots using both **Mistral AI** and **Hugging Face Transformers**.\n",
        "\n",
        "Keep experimenting, explore new models, and never stop learning. ðŸš€  \n",
        "The world of AI is full of opportunities â€” this is just the beginning!"
      ],
      "metadata": {
        "id": "OlYwhdGjGYFK"
      },
      "id": "OlYwhdGjGYFK"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}