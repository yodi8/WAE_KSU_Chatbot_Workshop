{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’» WAE Chatbot Workshop"
      ],
      "metadata": {
        "id": "XdXkYweB4fKe"
      },
      "id": "XdXkYweB4fKe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Required Libraries\n",
        "We will use:\n",
        "\n",
        "Gradio for building simple user interfaces.\n",
        "\n",
        "Mistral AI for accessing the language model.\n"
      ],
      "metadata": {
        "id": "NNGEtyAg50vF"
      },
      "id": "NNGEtyAg50vF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0eb939e-a7e6-42d9-a7ce-c61444c5dc62",
      "metadata": {
        "id": "e0eb939e-a7e6-42d9-a7ce-c61444c5dc62",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install mistralai\n",
        "!pip install gradio\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output() # Clear previous output for a cleaner notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b630861-a95e-4925-8525-d9461d3627ea",
      "metadata": {
        "id": "5b630861-a95e-4925-8525-d9461d3627ea"
      },
      "source": [
        "Our API is currently available through [La Plateforme](https://console.mistral.ai/). You need to activate payments on your account to enable your API keys. After activation, you can start using the `chat` endpoint to interact with models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Setting up the Mistral Model\n",
        "\n",
        "You will need to add your API Key from:  https://console.mistral.ai/api-keys/\n",
        "\n",
        "\n",
        "You can explore all available models here: https://docs.mistral.ai/getting-started/models/models_overview/\n",
        "\n"
      ],
      "metadata": {
        "id": "nBnI4U-ClTYM"
      },
      "id": "nBnI4U-ClTYM"
    },
    {
      "source": [
        "from mistralai import Mistral\n",
        "\n",
        "api_key = \"YOUR_API_KEY_HERE\" # ðŸ‘‰ Replace with your own API Key\n",
        "model = \"mistral-large-2402\"  # You can change the model if you want\n",
        "\n",
        "client = Mistral(api_key=api_key)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zHUx1oep34i4"
      },
      "id": "zHUx1oep34i4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple Chatbot using Mistral Model"
      ],
      "metadata": {
        "id": "gClQXFFt6ahF"
      },
      "id": "gClQXFFt6ahF"
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_mistral(message, history):\n",
        "    # Build the conversation context by combining the existing history with the new user message.\n",
        "    messages = history + [{\"role\": \"user\", \"content\": message}]\n",
        "\n",
        "    chat_response = client.chat.complete(\n",
        "        model=model,\n",
        "        messages=messages # Content from the user input\n",
        "    )\n",
        "    response = chat_response.choices[0].message.content\n",
        "\n",
        "    formatted_response = {\"role\": \"assistant\", \"content\": response}\n",
        "\n",
        "    # Return only the new assistant message.\n",
        "    return formatted_response"
      ],
      "metadata": {
        "id": "dEYiOJZglZhC"
      },
      "id": "dEYiOJZglZhC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Web Interface using Gradio"
      ],
      "metadata": {
        "id": "FBV_HsRC6ylF"
      },
      "id": "FBV_HsRC6ylF"
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "iface = gr.ChatInterface(\n",
        "    fn=chat_with_mistral, # Function to handle chat messages\n",
        "    title=\"Mistral AI Chatbot\",\n",
        "    description=\"Chat with the Mistral AI large language model.\",\n",
        "    theme=\"glass\", # Visual theme\n",
        "    type=\"messages\" # Set the input/output format to handle full chat messages\n",
        ")\n",
        "\n",
        "iface.launch(\n",
        "    debug=True, # Show detailed error messages if something goes wrong\n",
        "    share=True, # Create a public link to share your app with others\n",
        ")"
      ],
      "metadata": {
        "id": "M1WtoPUl0L57"
      },
      "id": "M1WtoPUl0L57",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ›  Exercise: Customize Your Chatbotâ€™s Personality!"
      ],
      "metadata": {
        "id": "GsgVEFFECVzp"
      },
      "id": "GsgVEFFECVzp"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROVIDE SOME STARTER CODE TO GUIDE STUDENTS"
      ],
      "metadata": {
        "id": "7KYGCjouDIaj"
      },
      "id": "7KYGCjouDIaj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **hint:**"
      ],
      "metadata": {
        "id": "tV2VPfZ6Crgl"
      },
      "id": "tV2VPfZ6Crgl"
    },
    {
      "cell_type": "markdown",
      "source": [
        " {\"role\": \"system\", \"content\": \"Your special instruction goes here.\"}"
      ],
      "metadata": {
        "id": "lcMNDWgfC70r"
      },
      "id": "lcMNDWgfC70r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Chatbot using Hugging Face Transformers"
      ],
      "metadata": {
        "id": "IWQWjPoM7oyx"
      },
      "id": "IWQWjPoM7oyx"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "gWSksJ8r7neV"
      },
      "id": "gWSksJ8r7neV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_name = \"facebook/opt-125m\"  # Choose a chatbot model from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name) # tokenizer: converts text into tokens that the model can understand\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name) # model: generates text responses based on the input tokens"
      ],
      "metadata": {
        "id": "Vpq4tK087WLF"
      },
      "id": "Vpq4tK087WLF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_huggingface(user_input, history):\n",
        "    # Tokenize the new user input\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # Build input by concatenating history (if any) and new user input\n",
        "    if history:\n",
        "        # Convert history list to tensor and add batch dimension\n",
        "        history_tensor = torch.tensor(history).unsqueeze(0)\n",
        "        bot_input_ids = torch.cat([history_tensor, new_user_input_ids], dim=-1)\n",
        "    else:\n",
        "        bot_input_ids = new_user_input_ids\n",
        "\n",
        "    # Create an attention mask (1 for non-eos_token, 0 for eos_token)\n",
        "    attention_mask = (bot_input_ids != tokenizer.eos_token_id).long()\n",
        "\n",
        "    # Generate a response\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=100, # Limit the response length\n",
        "        pad_token_id=tokenizer.eos_token_id, # Padding token (end of sequence)\n",
        "        attention_mask=attention_mask, # Mask to focus on relevant tokens\n",
        "        repetition_penalty=1.2,      # discourages repeating the same phrases\n",
        "        no_repeat_ngram_size=3,      # prevents repeating n-grams of a given size\n",
        "        do_sample=True,              # enables sampling (instead of greedy decoding)\n",
        "        top_p=0.9,                   # controls diversity\n",
        "        temperature=0.7              # Controls randomness in generation (lower = less random)\n",
        "    )\n",
        "\n",
        "\n",
        "    # Extract only the new tokens generated for the response\n",
        "    new_tokens = chat_history_ids[:, bot_input_ids.shape[-1]:][0]\n",
        "    response = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # Update history with both user input tokens and assistant's response tokens\n",
        "    history.extend(new_user_input_ids.tolist()[0])\n",
        "    history.extend(new_tokens.tolist())\n",
        "\n",
        "    return response, history\n"
      ],
      "metadata": {
        "id": "Vte4wypYqQ7e"
      },
      "id": "Vte4wypYqQ7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize conversation history as an empty list (no previous messages)\n",
        "chat_history = []\n",
        "\n",
        "# Example usage of the chatbot function with a user input\n",
        "user_input = \"Hello, how are you?\"\n",
        "response, chat_history = chat_with_huggingface(user_input, chat_history)\n",
        "\n",
        "# Print the chatbot's response\n",
        "print(f\"Chatbot: {response}\")"
      ],
      "metadata": {
        "id": "uNiWowBouIne"
      },
      "id": "uNiWowBouIne",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ›  Exercise: Try a Different Hugging Face Model!"
      ],
      "metadata": {
        "id": "C-0S3RlKFptj"
      },
      "id": "C-0S3RlKFptj"
    },
    {
      "cell_type": "code",
      "source": [
        "# PROVIDE SOME STARTER CODE TO GUIDE STUDENTS"
      ],
      "metadata": {
        "id": "PgRHAEDXFwpz"
      },
      "id": "PgRHAEDXFwpz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ‰ Congratulations\n",
        "\n",
        "Congratulations on completing the workshop!\n",
        "\n",
        "Today, you learned how to build chatbots using both **Mistral AI** and **Hugging Face Transformers**.\n",
        "\n",
        "Keep experimenting, explore new models, and never stop learning. ðŸš€  \n",
        "The world of AI is full of opportunities â€” this is just the beginning!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This file was created by **Eyad Alfaifi** and **Abdulmalik Alquifly**."
      ],
      "metadata": {
        "id": "OlYwhdGjGYFK"
      },
      "id": "OlYwhdGjGYFK"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}